{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "- Putting src data from BQ, into the workspace bucket\n",
    "- id_rsa key and placement in the ~ directory in Terra\n",
    "- Dirs in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "- Don't run all the cells at once without reviewing them, parameters toward the bottom, will need to be updated first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review/Edit all of the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = 'anvil_dbt_project'\n",
    "repo='git@github.com:NIH-NCPI/anvil_dbt_project.git' #The ssh version\n",
    "study_id = 'cmg_yale'\n",
    "\n",
    "ftd_schema = f'main_{study_id}_data'\n",
    "tgt_schema = f'main_{study_id}_tgt_data'\n",
    "\n",
    "gh_email = 'brenda.gutman@gmail.com'\n",
    "gh_user = 'brendagutman'\n",
    "\n",
    "other_files = ['terra_startup_script.sh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB and dbt setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from jinja2 import Template\n",
    "\n",
    "bucket = os.environ['WORKSPACE_BUCKET']\n",
    "con = duckdb.connect(\"/tmp/dbt.duckdb\")\n",
    "\n",
    "# Common paths\n",
    "home_dir = Path.cwd().parent.parent # Use parent because cwd is currently this nb's path, not home.\n",
    "repo_dir = home_dir / 'pipeline'\n",
    "pipeline_dir =  repo_dir / f'{pipeline}' # user editable location for the pipeline repo\n",
    "\n",
    "output_dir = repo_dir / f'output_data'\n",
    "output_study_dir = output_dir / study_id\n",
    "seeds_dir = pipeline_dir / 'seeds'\n",
    "data_dir = pipeline_dir / f\"data/{study_id}\" # place src data here\n",
    "profiles_dir = pipeline_dir / \"profiles.yml\" # locate file. Necessary to move dir for dbt/pipeline run.\n",
    "dbt_dir = home_dir / \".dbt\" # New loc for the profiles.yml\n",
    "ssh_dir = home_dir / \".ssh\"\n",
    "git_config_path = home_dir / \".gitconfig\"\n",
    "id_rsa_src = home_dir / \"id_rsa\"\n",
    "id_rsa_dest = ssh_dir / \"id_rsa\"\n",
    "bash_profile = home_dir / \".bash_profile\"\n",
    "terra_gitignore = home_dir / 'gitignore_global'\n",
    "bucket_study_dir = f'{bucket}/{study_id}' # ATM needs to be created manually via gcp and the src data inserted.\n",
    "\n",
    "print(f'INFO: Complete {seeds_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_dict(table, count):\n",
    "    file_list = []\n",
    "    for i in range(count):\n",
    "        if i == 0:\n",
    "            file = f'{table}_{\"0\" * 12}.csv'\n",
    "        else:\n",
    "            file = f'{table}_{\"0\" * (12 - len(str(i)))}{i}.csv'\n",
    "        file_list.append(file)\n",
    "    \n",
    "    return {table: file_list}\n",
    "\n",
    "def get_bucket_src_data_format_store(src_table_list):\n",
    "    '''\n",
    "    Data files are a special case. Get them from the bucket with this \n",
    "    function, NOT pull_study_files()\n",
    "    Data files should not be edited manually. If edits are required, \n",
    "    use the original queries with edits to store the new data in the bucket\n",
    "    '''\n",
    "    print('INFO: Start')\n",
    "\n",
    "    copy_data_from_bucket(bucket_study_dir, src_files, data_dir)\n",
    "\n",
    "    # Iterate over the dictionaries and process files\n",
    "    for file_dict in partial_file_dicts:  # Iterate over each dictionary in partial_file_dicts\n",
    "        for table, file_list in file_dict.items():  # Extract table name (key) and file list (value)\n",
    "            # Concatenate files for the current table\n",
    "            read_and_concat_files(file_list, data_dir, f'{data_dir}/{table}.csv')\n",
    "\n",
    "    # Rename the concatenated files\n",
    "    for table in src_table_list:  # Iterate over all table names\n",
    "        rename_file_single_dir(data_dir, f'{table}_000000000000.csv', f'{table}.csv')\n",
    "\n",
    "    # Remove all the files in the dictionaries\n",
    "    for file_dict in partial_file_dicts:  # Iterate over each dictionary in partial_file_dicts\n",
    "        for table, file_list in file_dict.items():\n",
    "#             print(file_list)\n",
    "            remove_file(file_list, data_dir)\n",
    "\n",
    "    print('INFO: Complete')\n",
    "    \n",
    "def setup_ssh():\n",
    "    # Create and configure ~/.ssh\n",
    "    if not ssh_dir.is_dir():\n",
    "        ssh_dir.mkdir(mode=0o700, exist_ok=True)\n",
    "        print(\"INFO: Created ~/.ssh directory.\")\n",
    "    ssh_config = ssh_dir / \"config\"\n",
    "    if not ssh_config.exists():\n",
    "        ssh_config.write_text(\n",
    "            \"\"\"# SSH configuration for GitHub\n",
    "Host github\n",
    "  HostName github.com\n",
    "  User git\n",
    "  IdentityFile ~/.ssh/id_rsa\n",
    "  IdentitiesOnly yes\n",
    "\"\"\"\n",
    "        )\n",
    "        ssh_config.chmod(0o600)\n",
    "        print(\"INFO: Created ~/.ssh/config file.\")\n",
    "        \n",
    " # Move id_rsa to ~/.ssh and set correct permissions\n",
    "    if id_rsa_src.exists():\n",
    "        os.system(f\"mv {id_rsa_src} {id_rsa_dest}\")\n",
    "        id_rsa_dest.chmod(0o600)\n",
    "        print(f\"INFO: Moved id_rsa to {id_rsa_dest} and set permissions to 600.\")\n",
    "\n",
    "    if not id_rsa_src.exists() and not id_rsa_dest.exists():\n",
    "        print(f\"WARNING: Make sure the private key is available.\")\n",
    "\n",
    "# See [docs](https://github.com/DataBiosphere/terra-examples/blob/main/best_practices/source_control/terra_source_control_cheatsheet.md#1-use-the-jupyter-console-to-upload-your-github-ssh-key-and-create-an-interactive-terminal-session) \n",
    "def setup_gh():\n",
    "    content1=f'''\n",
    "[user]\n",
    "        email = {gh_email}\n",
    "        name = {gh_user}\n",
    "[url \"git@github.com:\"]\n",
    "        insteadOf = https://github.com/\n",
    "\n",
    "    '''\n",
    "\n",
    "    with git_config_path.open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content1)\n",
    "\n",
    "    print(\"INFO: Edited ~/.gitconfig file.\")\n",
    "        \n",
    "def update_bash_profile():\n",
    "    \n",
    "    content1 = \"\"\"\n",
    "# Custom PS1 prompt with virtual environment display\n",
    "export PS1='\\\\[\\\\033[1;33m\\\\]${VIRTUAL_ENV:+(venv)} \\\\[\\\\033[1;36m\\\\]$(basename \"$PWD\")\\\\[\\\\033[00m\\\\]\\\\$ '\n",
    "\"\"\"\n",
    "\n",
    "    content2 = f\"\"\"\n",
    "# Add SSH private key\n",
    "eval \"ssh-add ~/.ssh/id_rsa\"\n",
    "\n",
    "# Alias to activate Python virtual environment\n",
    "alias activate=\"source /home/jupyter/venv-python3.12/bin/activate\"\n",
    "\n",
    "# Alias to setup ssh and permissions:\n",
    "alias setup_ssh=\"\n",
    "echo 'Assuming the id_rsa is in the {id_rsa_src} dir'\n",
    "eval 'mv {id_rsa_src} {id_rsa_dest}'\n",
    "eval 'chmod 600 ~/.ssh/id_rsa'\n",
    "\"\n",
    "\n",
    "# Setup dirs and clone the repo\n",
    "alias clone_repo=\"\n",
    "eval 'mkdir {repo_dir}'\n",
    "eval 'mkdir {pipeline_dir}'\n",
    "eval 'git clone {repo} {pipeline_dir}'\n",
    "eval 'mkdir {data_dir}'\n",
    "eval 'activate'\n",
    "eval 'cd {pipeline_dir}'\n",
    "eval 'mkdir {output_dir}'\n",
    "eval 'mkdir {output_study_dir}'\n",
    "\"\n",
    "\n",
    "# Alias to dbt prep file system:\n",
    "alias setup_data=\"\n",
    "eval 'mkdir {dbt_dir}'\n",
    "eval 'cp {profiles_dir} {dbt_dir}'\n",
    "\"\n",
    "\n",
    "# Alias to clean and compile pipeline:\n",
    "alias r_dbt=\"\n",
    "eval 'dbt clean'\n",
    "eval 'dbt deps'\n",
    "\"\n",
    "\n",
    "echo 'Alias are: activate, r_dbt, setup_ssh clone_repo, setup_data'\n",
    "\n",
    "export LOCUTUS_LOGLEVEL='INFO'\n",
    "\n",
    "\"\"\"\n",
    "    with bash_profile.open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content1 + \"\\n\" + content2)\n",
    "        \n",
    "    print(\"INFO: Content successfully added to ~/.bash_profile.\")\n",
    "\n",
    "    print(\"INFO: To apply changes, run: source ~/.bash_profile\")\n",
    "\n",
    "def stop_gitignoring_sql_files():\n",
    "    content = \"\"\"\n",
    "!*.sql\n",
    "\"\"\"\n",
    "    with terra_gitignore.open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content + \"\\n\")\n",
    "    print(\"INFO: Content successfully added to ~/gitignore_global\")\n",
    "    \n",
    "    \n",
    "def run_initial_setup():\n",
    "    '''\n",
    "    Run the setup functions\n",
    "    '''\n",
    "    setup_ssh() # Required first time env setup\n",
    "    setup_gh() # Required first time env setup\n",
    "    update_bash_profile()\n",
    "    stop_gitignoring_sql_files()\n",
    "def copy_data_from_bucket(bucket_study_dir, file_list, output_dir):\n",
    "    for file in file_list:\n",
    "#         TODO: checkout rsync https://google-cloud-how-to.smarthive.io/buckets/rsync\n",
    "        !gsutil cp {bucket_study_dir}/{file} {output_dir}\n",
    "        print(f'INFO: Copied {file} to {output_dir}') \n",
    "\n",
    "def copy_data_to_bucket(bucket_study_dir, file_list, input_dir):\n",
    "    for file in file_list:\n",
    "        !gsutil cp {input_dir} {bucket_study_dir}/{file}\n",
    "        print(f'INFO: Copied {file} to the bucket') \n",
    "        \n",
    "# Read and concatenate all files\n",
    "def read_and_concat_files(file_list, input_dir, output_dir):\n",
    "    dfs = [pd.read_csv(f'{input_dir}/{file}') for file in file_list] \n",
    "    combined_subject = pd.concat(dfs, ignore_index=True)\n",
    "    combined_subject.to_csv(output_dir, index=False)\n",
    "    \n",
    "def rename_file_single_dir(d_dir, input_fn, output_fn):\n",
    "    # clean up data_dir\n",
    "    !mv {d_dir}/{input_fn} {d_dir}/{output_fn}\n",
    "    \n",
    "def remove_file(file_list, d_dir):\n",
    "    for file in file_list:\n",
    "        !rm {d_dir}/{file}\n",
    "        print(f'INFO: Processsed: {file}')\n",
    "    \n",
    "    # Save the files before shutting down an environment\n",
    "def store_study_files():\n",
    "    \"\"\" Store defined files in the bucket. These will persist when env is shutdown.\"\"\"\n",
    "    for file in study_files:\n",
    "        !gsutil cp {data_dir}/{file} {bucket_study_dir}\n",
    "    for file in seeds_files:\n",
    "        !gsutil cp {seeds_dir}/{file} {bucket_study_dir}\n",
    "        \n",
    "def get_study_files():\n",
    "    \"\"\" Pull study files from where they are saved in the workspace bucket.\n",
    "    SHOULD NOT get datafiles. Run get_bucket_src_data_format_store for those.\"\"\"\n",
    "    for file in study_files:\n",
    "        !gsutil cp  {bucket_study_dir}/{file} {data_dir}/{file}\n",
    "    for file in seeds_files:\n",
    "        !gsutil cp  {bucket_study_dir}/{file} {seeds_dir}/{file}\n",
    "    for file in other_files:\n",
    "        !gsutil cp  {bucket}/{file} {data_dir}/{file}\n",
    "        \n",
    "    \n",
    "# Export functions       \n",
    "def get_tables_from_schema(schema):\n",
    "    '''\n",
    "    Get tables from a duckdb dataset. \n",
    "    '''\n",
    "    result = con.execute(f\"\"\"\n",
    "    SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema}'\n",
    "    \"\"\")\n",
    "    r = pd.DataFrame(result.fetchall(), columns=[col[0] for col in result.description])\n",
    "    return r['table_name'].to_list()\n",
    "\n",
    "def tables_to_output_dir(tables):\n",
    "    for t in tables:\n",
    "        name = Path(t).stem.replace(f'tgt_','')\n",
    "        t = con.execute( f\"COPY (SELECT * FROM {tgt_schema}.{t}) TO '{output_study_dir}/{name}.csv' (HEADER, DELIMITER ',')\").fetchall()\n",
    "        print(name)\n",
    "\n",
    "def harmonized_to_bucket(tables):\n",
    "    for t in tables:\n",
    "        name = Path(t).stem.replace(f'tgt_','')\n",
    "        !gsutil cp {output_study_dir}/{name}.csv {bucket}/harmonized/{study_id}\n",
    "        print(name)\n",
    "\n",
    "\n",
    "def copy_to_csv_and_export_to_bucket():    \n",
    "    '''\n",
    "    Get the tables that you want to export to csv.\n",
    "    Then export to csv in the output dir\n",
    "    '''\n",
    "    tgt_tables = get_tables_from_schema(tgt_schema)\n",
    "\n",
    "    tables_to_output_dir(tgt_tables)\n",
    "    display('Tables sent to output.')\n",
    "    \n",
    "    harmonized_to_bucket(tgt_tables)\n",
    "    display('csvs sent to bucket')\n",
    "    \n",
    "def convert_csv_to_utf8(input_file_path, output_filepath, delimiter, encoding):\n",
    "    df = pd.read_csv(input_file_path, encoding=encoding, delimiter=delimiter, quoting=3)\n",
    "    df.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "    print(f\"Converted CSV saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study specific parameters\n",
    "- Add another if starting a new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Template for adding a new study\n",
    "\"\"\"\n",
    "if study_id == '______':\n",
    "    src_table_list = ['______','______']\n",
    "\n",
    "    # files needing concat\n",
    "    subject_pfiles = create_file_dict('______', ______)\n",
    "    sample_pfiles = create_file_dict('______', ______)\n",
    "    \n",
    "    partial_file_dicts = [ _______pfiles, _______pfiles]\n",
    "    src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "    src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['_______dd.csv',\n",
    "                   '_______dd.csv'\n",
    "                  ]\n",
    "\n",
    "    seeds_files = [] # Insert seed files if applicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  CMG-BH specific\n",
    "Only edit if working on cmg_bh\n",
    "\"\"\"\n",
    "if study_id == 'cmg_bh':\n",
    "    src_table_list = ['subject','sample']\n",
    "\n",
    "    # files needing concat\n",
    "    subject_pfiles = create_file_dict('subject', 2)\n",
    "    sample_pfiles = create_file_dict('sample', 1)\n",
    "    \n",
    "    partial_file_dicts = [subject_pfiles, sample_pfiles]\n",
    "    src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "    src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['subject_dd.csv',\n",
    "                   'sample_dd.csv'\n",
    "                  ]\n",
    "\n",
    "    seeds_files = ['cmg_bh_annotations_code.csv', 'subject_mappings.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  CMG-Yale specific\n",
    "Only edit if working on cmg_yale\n",
    "\"\"\"\n",
    "if study_id == 'cmg_yale':\n",
    "    src_table_list = ['subject','sample','anvil_dataset','sequencing','family']\n",
    "\n",
    "    # files needing concat\n",
    "    subject_pfiles = create_file_dict('subject', 9)\n",
    "    sample_pfiles = create_file_dict('sample', 9)\n",
    "    anvil_dataset_pfiles = create_file_dict('anvil_dataset', 9) \n",
    "    sequencing_pfiles = create_file_dict('sequencing', 4) \n",
    "    family_pfiles = create_file_dict('family', 9) \n",
    "    \n",
    "    partial_file_dicts = [subject_pfiles, sample_pfiles, anvil_dataset_pfiles, sequencing_pfiles, family_pfiles]\n",
    "    src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "    src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['subject_dd.csv',\n",
    "                   'sample_dd.csv',\n",
    "                   'cmg_yale_study.yaml',\n",
    "                   'ftd_study.yaml',\n",
    "                   'Yale_CMG_Master_DD.csv'\n",
    "                  ]\n",
    "    \n",
    "    seeds_files = ['RoleCodeValueSet.csv', 'kin-to-fhir-FamilyMember.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run functions\n",
    "- Enable functions and run one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If starting a new pipeline env\n",
    "- After putting your Private id_rsa key file in the home dir in Terra\n",
    "1. Set up GH and terminal configurations\n",
    "- Run 'run_initial_setup' in this cell\n",
    "- Go to terminal and run:\n",
    "    - 'source ~/.bash_profile' - A list of available commands should show up\n",
    "    - 'setup_ssh'\n",
    "    - 'clone_repo'\n",
    "    - 'setup_data'\n",
    "- At this point you should be able to connect to GitHub and swap branches\n",
    "'''\n",
    "# run_initial_setup()\n",
    "\n",
    "\n",
    "'''\n",
    "Get the dds and config files from the bucket\n",
    "'''\n",
    "# get_study_files() \n",
    "\n",
    "\n",
    "'''\n",
    "Get the src data files from the bucket\n",
    "'''\n",
    "# get_bucket_src_data_format_store(src_table_list)\n",
    "\n",
    "\n",
    "'''\n",
    "Put study files into the bucket.\n",
    "'''\n",
    "# store_study_files()\n",
    "\n",
    "\n",
    "'''\n",
    "Export tgt data to csvs in the output dir. Then send the files to the harmonized dir in the bucket\n",
    "'''\n",
    "# copy_to_csv_and_export_to_bucket()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Convert files in data dir into utf-8. Add to the appropriate list, to save the changes in the bucket.\n",
    "\"\"\"\n",
    "# input_filepath = f'{seeds_dir}/value_sets/RoleCodeValueSet.csv'\n",
    "# output_filepath = f'{seeds_dir}/value_sets/RoleCodeValueSet.csv'\n",
    "# delimiter = '\\t'\n",
    "# encoding = 'latin1'\n",
    "# convert_csv_to_utf8(input_filepath, output_filepath, delimiter, encoding)\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "venv-python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
