{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "- Putting src data from BQ, into the workspace bucket\n",
    "- id_rsa key and placement in the ~ directory in Terra\n",
    "- Dirs in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "\n",
    "- Run this notebook from the Terra Analysis page. The copy stored in the repository is kept for source control.\n",
    "- Don't run all the cells at once without reviewing them, parameters toward the bottom, will need to be updated first.\n",
    "\n",
    "\n",
    "import separate files to bucket via BQ \n",
    "then run pipeline_helpers to get them into /data\n",
    "<!-- then run the import macro to get them into duckdb -->\n",
    "duckdb can pull from a path. -- can skip import if they exist in data dir.\n",
    "\n",
    "dbt run-operation register_external_sources --args '{\"fq_tablename\":\"sample_ANVIL_CMG_YALE_DS_MC_20221026_ANV5_202409302315_000000000000\",\"src_loc\":\"data/cmg_yale/sample_ANVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review/Edit all of the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = 'anvil_dbt_project'\n",
    "repo='git@github.com:NIH-NCPI/anvil_dbt_project.git' #The ssh version\n",
    "study_id = 'cmg_yale'\n",
    "org_id = 'anvil'\n",
    "dbt_repo = 'anvil_dbt_project'\n",
    "tgt_model_id = 'tgt_consensus_a'\n",
    "\n",
    "ftd_schema = f'main_{study_id}_data'\n",
    "tgt_schema = f'main_{study_id}_tgt_data'\n",
    "\n",
    "gh_email = ''\n",
    "gh_user = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB and dbt setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from jinja2 import Template\n",
    "import sys\n",
    "\n",
    "bucket = os.environ['WORKSPACE_BUCKET']\n",
    "con = duckdb.connect(\"/tmp/dbt.duckdb\")\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "from scripts.general import get_all_paths as get_all_paths\n",
    "paths = get_all_paths(study_id, dbt_repo, org_id, tgt_model_id, src_data_path=None)\n",
    "\n",
    "# print(f'INFO: Complete {repo_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_dict(table, count):\n",
    "    file_list = []\n",
    "    for i in range(count):\n",
    "        if i == 0:\n",
    "            file = f'{table}_{\"0\" * 12}.csv'\n",
    "        else:\n",
    "            file = f'{table}_{\"0\" * (12 - len(str(i)))}{i}.csv'\n",
    "        file_list.append(file)\n",
    "    \n",
    "    return {table: file_list}\n",
    "\n",
    "def get_bucket_src_data_format_store(src_table_list):\n",
    "    '''\n",
    "    Data files are a special case. Get them from the bucket with this \n",
    "    function, NOT pull_study_files()\n",
    "    Data files should not be edited manually. If edits are required, \n",
    "    use the original queries with edits to store the new data in the bucket\n",
    "    '''\n",
    "    print('INFO: Start')\n",
    "\n",
    "    copy_data_from_bucket(paths['bucket_study_dir'], src_files, paths['src_data_dir'])\n",
    "\n",
    "    # Iterate over the dictionaries and process files\n",
    "    for file_dict in partial_file_dicts:  # Iterate over each dictionary in partial_file_dicts\n",
    "        for table, file_list in file_dict.items():  # Extract table name (key) and file list (value)\n",
    "            # Concatenate files for the current table\n",
    "            read_and_concat_files(file_list, paths['src_data_dir'], f'{paths['src_data_dir']}/{table}_combined.csv')\n",
    "\n",
    "    # Rename the concatenated files\n",
    "    for table in src_table_list:  # Iterate over all table names\n",
    "        rename_file_single_dir(paths['src_data_dir'], f'{table}_combined.csv', f'{table}.csv')\n",
    "\n",
    "    # Remove all the files in the dictionaries\n",
    "    for file_dict in partial_file_dicts:  # Iterate over each dictionary in partial_file_dicts\n",
    "        for table, file_list in file_dict.items():\n",
    "            remove_file(file_list, paths['src_data_dir'])\n",
    "\n",
    "    print('INFO: Complete')\n",
    "    \n",
    "def setup_ssh():\n",
    "    # Create and configure ~/.ssh\n",
    "    if not paths['ssh_dir'].is_dir():\n",
    "        paths['ssh_dir'].mkdir(mode=0o700, exist_ok=True)\n",
    "        print(\"INFO: Created ~/.ssh directory.\")\n",
    "    ssh_config = paths['ssh_dir'] / \"config\"\n",
    "    if not ssh_config.exists():\n",
    "        ssh_config.write_text(\n",
    "            \"\"\"# SSH configuration for GitHub\n",
    "Host github\n",
    "  HostName github.com\n",
    "  User git\n",
    "  IdentityFile ~/.ssh/id_rsa\n",
    "  IdentitiesOnly yes\n",
    "\"\"\"\n",
    "        )\n",
    "        ssh_config.chmod(0o600)\n",
    "        print(\"INFO: Created ~/.ssh/config file.\")\n",
    "        \n",
    " # Move id_rsa to ~/.ssh and set correct permissions\n",
    "    if id_rsa_src.exists():\n",
    "        os.system(f\"mv {id_rsa_src} {id_rsa_dest}\")\n",
    "        id_rsa_dest.chmod(0o600)\n",
    "        print(f\"INFO: Moved id_rsa to {id_rsa_dest} and set permissions to 600.\")\n",
    "\n",
    "    if not id_rsa_src.exists() and not id_rsa_dest.exists():\n",
    "        print(f\"WARNING: Make sure the private key is available.\")\n",
    "\n",
    "# See [docs](https://github.com/DataBiosphere/terra-examples/blob/main/best_practices/source_control/terra_source_control_cheatsheet.md#1-use-the-jupyter-console-to-upload-your-github-ssh-key-and-create-an-interactive-terminal-session) \n",
    "def setup_gh():\n",
    "    content1=f'''\n",
    "[user]\n",
    "        email = {gh_email}\n",
    "        name = {gh_user}\n",
    "[url \"git@github.com:\"]\n",
    "        insteadOf = https://github.com/\n",
    "\n",
    "    '''\n",
    "\n",
    "    with git_config_path.open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content1)\n",
    "\n",
    "    print(\"INFO: Edited ~/.gitconfig file.\")\n",
    "        \n",
    "def update_bash_profile():\n",
    "    \n",
    "    content1 = \"\"\"\n",
    "# Custom PS1 prompt with virtual environment display\n",
    "export PS1='\\\\[\\\\033[1;33m\\\\]${VIRTUAL_ENV:+(venv)} \\\\[\\\\033[1;36m\\\\]$(basename \"$PWD\")\\\\[\\\\033[00m\\\\]\\\\$ '\n",
    "\"\"\"\n",
    "\n",
    "    content2 = f\"\"\"\n",
    "# Add SSH private key\n",
    "eval \"ssh-add ~/.ssh/id_rsa\"\n",
    "\n",
    "# Alias to activate Python virtual environment\n",
    "alias activate=\"source /home/jupyter/venv-python3.12/bin/activate\"\n",
    "\n",
    "# Alias to setup ssh and permissions:\n",
    "alias setup_ssh=\"\n",
    "echo 'Assuming the id_rsa is in the {paths['id_rsa_src']} dir'\n",
    "eval 'mv {paths['id_rsa_src']} {paths['id_rsa_dest']}'\n",
    "eval 'chmod 600 ~/.ssh/id_rsa'\n",
    "\"\n",
    "\n",
    "# Setup dirs and clone the repo\n",
    "alias clone_repo=\"\n",
    "eval 'mkdir {paths['pipeline_dir']}'\n",
    "eval 'mkdir {paths['repo_home_dir']}'\n",
    "eval 'git clone {repo} {paths['repo_home_dir']}'\n",
    "eval 'mkdir paths['src_data_dir']'\n",
    "eval 'activate'\n",
    "eval 'cd {paths['repo_home_dir']}'\n",
    "eval 'mkdir {paths['output_dir']}'\n",
    "eval 'mkdir {paths['outputz_study_dir']}'\n",
    "\"\n",
    "\n",
    "# Alias to dbt prep file system:\n",
    "alias setup_data=\"\n",
    "eval 'mkdir {paths['dbt_dir']}'\n",
    "eval 'cp {paths['profiles_path_root']} {paths['profiles_path_home']}'\n",
    "\"\n",
    "\n",
    "# Alias to clean and compile pipeline:\n",
    "alias r_dbt=\"\n",
    "eval 'dbt clean'\n",
    "eval 'dbt deps'\n",
    "\"\n",
    "\n",
    "echo 'Alias are: activate, r_dbt, setup_ssh clone_repo, setup_data'\n",
    "\n",
    "export LOCUTUS_LOGLEVEL='INFO'\n",
    "\n",
    "\"\"\"\n",
    "    with bash_profile.open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content1 + \"\\n\" + content2)\n",
    "        \n",
    "    print(\"INFO: Content successfully added to ~/.bash_profile.\")\n",
    "\n",
    "    print(\"INFO: To apply changes, run: source ~/.bash_profile\")\n",
    "\n",
    "def stop_gitignoring_sql_files():\n",
    "    content = \"\"\"\n",
    "!*.sql\n",
    "!*.yml\n",
    "\"\"\"\n",
    "    with paths['terra_gitignore'].open(\"a\") as file:\n",
    "        file.write(\"\\n\" + content + \"\\n\")\n",
    "    print(\"INFO: Content successfully added to ~/gitignore_global\")\n",
    "    \n",
    "    \n",
    "def run_initial_setup():\n",
    "    '''\n",
    "    Run the setup functions\n",
    "    '''\n",
    "    setup_ssh() # Required first time env setup\n",
    "    setup_gh() # Required first time env setup\n",
    "    update_bash_profile()\n",
    "    stop_gitignoring_sql_files()\n",
    "def copy_data_from_bucket(bucket_study_dir, file_list, output_dir):\n",
    "    for file in file_list:\n",
    "#         TODO: checkout rsync https://google-cloud-how-to.smarthive.io/buckets/rsync\n",
    "        !gsutil cp {bucket_study_dir}/{file} {output_dir}\n",
    "        print(f'INFO: Copied {file} to {output_dir}') \n",
    "\n",
    "def copy_data_to_bucket(bucket_study_dir, file_list, input_dir):\n",
    "    for file in file_list:\n",
    "        !gsutil cp {input_dir} {bucket_study_dir}/{file}\n",
    "        print(f'INFO: Copied {file} to the bucket') \n",
    "        \n",
    "# Read and concatenate all files\n",
    "def read_and_concat_files(file_list, input_dir, output_dir):\n",
    "    dfs = [pd.read_csv(f'{input_dir}/{file}') for file in file_list] \n",
    "    combined_subject = pd.concat(dfs, ignore_index=True)\n",
    "    combined_subject.to_csv(output_dir, index=False)\n",
    "    \n",
    "def rename_file_single_dir(d_dir, input_fn, output_fn):\n",
    "    # clean up data_dir\n",
    "    !mv {d_dir}/{input_fn} {d_dir}/{output_fn}\n",
    "    \n",
    "def remove_file(file_list, d_dir):\n",
    "    for file in file_list:\n",
    "        !rm {d_dir}/{file}\n",
    "        print(f'INFO: Processsed: {file}')\n",
    "    \n",
    "    # Save the files before shutting down an environment\n",
    "def store_study_files():\n",
    "    \"\"\" Store defined files in the bucket. These will persist when env is shutdown.\"\"\"\n",
    "    for file in study_files:\n",
    "        !gsutil cp {paths['src_data_dir']}/{file} {paths['bucket_study_dir']}\n",
    "    for file in seeds_files:\n",
    "        !gsutil cp {paths['seeds_dir']}/{file} {paths['bucket_study_dir']}\n",
    "        \n",
    "def get_study_files():\n",
    "    \"\"\" Pull study files from where they are saved in the workspace bucket.\n",
    "    SHOULD NOT get datafiles. Run get_bucket_src_data_format_store for those.\"\"\"\n",
    "    for file in study_files:\n",
    "        !gsutil cp  {paths['bucket_study_dir']}/{file} {paths['src_data_dir']}/{file}\n",
    "    for file in seeds_files:\n",
    "        !gsutil cp  {paths['bucket_study_dir']}/{file} {paths['seeds_dir']}/{file}\n",
    "    for file in other_files:\n",
    "        !gsutil cp  {paths['bucket']}/{file} {paths['src_data_dir']}/{file}\n",
    "        \n",
    "    \n",
    "# Export functions       \n",
    "def get_tables_from_schema(schema):\n",
    "    '''\n",
    "    Get tables from a duckdb dataset. \n",
    "    '''\n",
    "    result = con.execute(f\"\"\"\n",
    "    SELECT table_name FROM information_schema.tables WHERE table_schema = '{schema}'\n",
    "    \"\"\")\n",
    "    r = pd.DataFrame(result.fetchall(), columns=[col[0] for col in result.description])\n",
    "    return r['table_name'].to_list()\n",
    "\n",
    "def tables_to_output_dir(tables):\n",
    "    for t in tables:\n",
    "        name = Path(t).stem.replace(f'tgt_','')\n",
    "        t = con.execute( f\"COPY (SELECT * FROM {tgt_schema}.{t}) TO '{paths['output_study_dir']}/{name}.csv' (HEADER, DELIMITER ',')\").fetchall()\n",
    "        print(name)\n",
    "\n",
    "def harmonized_to_bucket(tables):\n",
    "    for t in tables:\n",
    "        name = Path(t).stem.replace(f'tgt_','')\n",
    "        !gsutil cp {paths['output_study_dir']}/{name}.csv {paths['bucket']}/harmonized/{study_id}\n",
    "        print(name)\n",
    "\n",
    "\n",
    "def copy_to_csv_and_export_to_bucket():    \n",
    "    '''\n",
    "    Get the tables that you want to export to csv.\n",
    "    Then export to csv in the output dir\n",
    "    '''\n",
    "    tgt_tables = get_tables_from_schema(tgt_schema)\n",
    "\n",
    "    tables_to_output_dir(tgt_tables)\n",
    "    display('Tables sent to output.')\n",
    "    \n",
    "    harmonized_to_bucket(tgt_tables)\n",
    "    display('csvs sent to bucket')\n",
    "    \n",
    "def convert_csv_to_utf8(input_file_path, output_filepath, delimiter, encoding):\n",
    "    df = pd.read_csv(input_file_path, encoding=encoding, delimiter=delimiter, quoting=3)\n",
    "    df.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "    print(f\"Converted CSV saved to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study specific parameters\n",
    "- Add another if starting a new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Template for adding a new study\n",
    "\"\"\"\n",
    "if study_id == '______':\n",
    "    src_table_list = ['______','______']\n",
    "\n",
    "    # files needing concat\n",
    "    subject_pfiles = create_file_dict('______', ______)\n",
    "    sample_pfiles = create_file_dict('______', ______)\n",
    "    \n",
    "    partial_file_dicts = [ _______pfiles, _______pfiles]\n",
    "    src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "    src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['_______dd.csv',\n",
    "                   '_______dd.csv'\n",
    "                  ]\n",
    "\n",
    "    seeds_files = [] # Insert seed files if applicable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  CMG-BH specific\n",
    "Only edit if working on cmg_bh\n",
    "\"\"\"\n",
    "if study_id == 'cmg_bh':\n",
    "    src_table_list = ['subject','sample']\n",
    "\n",
    "    # files needing concat\n",
    "    subject_pfiles = create_file_dict('subject', 2)\n",
    "    sample_pfiles = create_file_dict('sample', 1)\n",
    "    \n",
    "    partial_file_dicts = [subject_pfiles, sample_pfiles]\n",
    "    src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "    src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['subject_dd.csv',\n",
    "                   'sample_dd.csv'\n",
    "                  ]\n",
    "\n",
    "    seeds_files = ['cmg_bh_annotations_code.csv', 'subject_mappings.csv']\n",
    "    other_files = ['terra_startup_script.sh']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  CMG-Yale specific\n",
    "Only edit if working on cmg_yale\n",
    "\"\"\"\n",
    "if study_id == 'cmg_yale':\n",
    "    src_table_list = [\n",
    "                      'subject'\n",
    "                      ,'sample'\n",
    "                      ,'anvil_dataset'\n",
    "                      ,'sequencing'\n",
    "                      ,'family'\n",
    "                      ,'file_inventory'\n",
    "                     ]\n",
    "    \n",
    "    tables = ['anvil_dataset']\n",
    "\n",
    "    datasets = ['datarepo-c2a2b724.ANVIL_CMG_YALE_DS_MC_20221026_ANV5_202409302315',\n",
    "               'datarepo-5c7f0d2a.ANVIL_CMG_Yale_HMB_IRB_20240113_ANV5_202410011846',\n",
    "               'datarepo-fe056168.ANVIL_CMG_Yale_DS_RD_20240113_ANV5_202410011804',\n",
    "               'datarepo-f961f617.ANVIL_CMG_YALE_DS_RARED_20221020_ANV5_202409251714',\n",
    "               'datarepo-06182245.ANVIL_CMG_Yale_DS_THAL_IRB_20240113_ANV5_202410011814',\n",
    "               'datarepo-35779fe0.ANVIL_CMG_Yale_HMB_20221020_ANV5_202410011825',\n",
    "               'datarepo-cebe6de0.ANVIL_CMG_Yale_HMB_GSO_20221020_ANV5_202410011834',\n",
    "               'datarepo-5d222190.ANVIL_CMG_Yale_DS_BPEAKD_20240113_ANV5_202410011754',\n",
    "               'datarepo-2b98851b.ANVIL_CMG_Yale_GRU_20221020_ANV5_202507091800'\n",
    "              ]\n",
    "\n",
    "    src_table_list = []\n",
    "    for table in tables:\n",
    "        for ds in datasets:\n",
    "            tablename = ds.split('.', 1)[1]\n",
    "            src_table_list.append(f\"{table}_{tablename}_000000000000.csv\")\n",
    "\n",
    "    # files needing concat\n",
    "#     subject_pfiles = create_file_dict('subject', 9)\n",
    "#     sample_pfiles = create_file_dict('sample', 9)\n",
    "#     anvil_dataset_pfiles = create_file_dict('anvil_dataset', 9) \n",
    "#     sequencing_pfiles = create_file_dict('sequencing', 4) \n",
    "#     family_pfiles = create_file_dict('family', 9) \n",
    "#     file_inventory_pfiles = create_file_dict('file_inventory', 9) \n",
    "    \n",
    "#     partial_file_dicts = [subject_pfiles\n",
    "#                           , sample_pfiles\n",
    "#                           , anvil_dataset_pfiles\n",
    "#                           , sequencing_pfiles\n",
    "#                           , family_pfiles\n",
    "#                           , file_inventory_pfiles\n",
    "#                          ]\n",
    "#     src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "#     src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "    # Define the files to store\n",
    "    study_files = ['subject_dd.csv',\n",
    "                   'sample_dd.csv',\n",
    "                   'family_dd.csv',\n",
    "                   'sequencing_dd.csv',\n",
    "                   'file_inventory_dd.csv',\n",
    "                   'cmg_yale_study.yaml',\n",
    "                   'ftd_study.yaml',\n",
    "                   'Yale_CMG_Master_DD.csv'\n",
    "                  ]\n",
    "    \n",
    "    seeds_files = []\n",
    "    other_files = ['terra_startup_script.sh']\n",
    "    nbs = ['pipeline_helpers.ipynb', 'raw_data_validation.ipynb']\n",
    "#     print(src_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Template for adding a new study\n",
    "\"\"\"\n",
    "if study_id == '______':\n",
    "    src_files = []\n",
    "    src_table_list = []\n",
    "    if study_id == 'study_here':\n",
    "        src_table_list = ['table1','table2']\n",
    "\n",
    "        \"\"\"\n",
    "        Handles files in the bucket needing concat(pfiles - Partial files).\n",
    "        Ex: ['subject_000000000001','subject_000000000002']\n",
    "        Optional. Depends on how the data was imported into the workspace bucket.\n",
    "        Comment them out if not needed.\n",
    "        \"\"\"\n",
    "        subject_pfiles = create_file_dict('{table}', max_file_n_plus_one)\n",
    "        sample_pfiles = create_file_dict('{table}', max_file_n_plus_one)\n",
    "\n",
    "        partial_file_dicts = [] # List of the pfiles created above. Must have the suffix '_pfiles'. \n",
    "\n",
    "\n",
    "        # No edits necessary.\n",
    "        if len(partial_file_dicts) != 0:\n",
    "            src_files = [file for file_dict in partial_file_dicts for file_list in file_dict.values() for file in file_list]\n",
    "            src_table_list = [key for file_dict in partial_file_dicts for key in file_dict.keys()]\n",
    "\n",
    "\n",
    "\n",
    "        # Define the files to store in the bucket that exist in the data/{study_id} dir.\n",
    "        study_files = ['_______dd.csv',\n",
    "                       '_______dd.csv'\n",
    "                      ]\n",
    "\n",
    "        seeds_files = [] # Insert seed files to send to the bucket if applicable\n",
    "\n",
    "        # Define the files to pull from the bucket(not from a study dir inside the bucket), placed in the data/{study_id} dir\n",
    "        other_files = [] \n",
    "\n",
    "        # Define notebooks from the analysis dir that need to be updated in the GH repo\n",
    "        nbs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run functions\n",
    "- Enable functions and run one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If starting a new pipeline env\n",
    "- After putting your Private id_rsa key file in the home dir in Terra\n",
    "1. Set up GH and terminal configurations\n",
    "- Run 'run_initial_setup' in this cell\n",
    "- Go to terminal and run:\n",
    "    - 'source ~/.bash_profile' - A list of available commands should show up\n",
    "    - 'setup_ssh'\n",
    "    - 'clone_repo'\n",
    "    - 'setup_data'\n",
    "- At this point you should be able to connect to GitHub and swap branches\n",
    "'''\n",
    "# run_initial_setup()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Get the dds and config files from the bucket\n",
    "'''\n",
    "# get_study_files() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Get the src data files from the bucket\n",
    "'''\n",
    "# get_bucket_src_data_format_store(src_table_list)\n",
    "\n",
    "\"\"\"\n",
    "Run the following if you only want to pull in the data files from the bucket. Run this \n",
    "if there are no 'partial' files to combine.\n",
    "\"\"\"\n",
    "# copy_data_from_bucket(paths['bucket_study_dir'], src_table_list, paths['src_data_dir'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Put study files into the bucket.\n",
    "'''\n",
    "# store_study_files()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Export tgt data to csvs in the output dir. Then send the files to the harmonized dir in the bucket\n",
    "'''\n",
    "# copy_to_csv_and_export_to_bucket()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Convert files in data dir into utf-8. Add to the appropriate list, to save the changes in the bucket.\n",
    "\"\"\"\n",
    "# input_filepath = f'{seeds_dir}/value_sets/RoleCodeValueSet.csv'\n",
    "# output_filepath = f'{seeds_dir}/value_sets/RoleCodeValueSet.csv'\n",
    "# delimiter = '\\t'\n",
    "# encoding = 'latin1'\n",
    "# convert_csv_to_utf8(input_filepath, output_filepath, delimiter, encoding)\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['ANVIL_CMG_YALE_DS_MC_20221026_ANV5_202409302315',\n",
    "           'ANVIL_CMG_Yale_HMB_IRB_20240113_ANV5_202410011846',\n",
    "           'ANVIL_CMG_Yale_DS_RD_20240113_ANV5_2024100118045',\n",
    "           'ANVIL_CMG_YALE_DS_RARED_20221020_ANV5_202409251714',\n",
    "           'ANVIL_CMG_Yale_DS_THAL_IRB_20240113_ANV5_202410011814',\n",
    "           'ANVIL_CMG_Yale_HMB_20221020_ANV5_202410011825',\n",
    "           'ANVIL_CMG_Yale_HMB_GSO_20221020_ANV5_202410011834',\n",
    "           'ANVIL_CMG_Yale_DS_BPEAKD_20240113_ANV5_202410011754',\n",
    "           'ANVIL_CMG_Yale_GRU_20221020_ANV5_202507091800'\n",
    "          ]\n",
    "\n",
    "for file in samples:\n",
    "    print(f\"\"\"\n",
    "    EXPORT DATA OPTIONS(uri='{bucket}/cmg_yale/sample_*.csv', format='CSV', overwrite=true ,header=true) AS \n",
    "    (\n",
    "    SELECT *\n",
    "    FROM `datarepo-5c7f0d2a.{file}.sample`\n",
    "    );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['ANVIL_CMG_YALE_DS_MC_20221026_ANV5_202409302315',\n",
    "           'ANVIL_CMG_Yale_HMB_IRB_20240113_ANV5_202410011846',\n",
    "           'ANVIL_CMG_Yale_DS_RD_20240113_ANV5_2024100118045',\n",
    "           'ANVIL_CMG_YALE_DS_RARED_20221020_ANV5_202409251714',\n",
    "           'ANVIL_CMG_Yale_DS_THAL_IRB_20240113_ANV5_202410011814',\n",
    "           'ANVIL_CMG_Yale_HMB_20221020_ANV5_202410011825',\n",
    "           'ANVIL_CMG_Yale_HMB_GSO_20221020_ANV5_202410011834',\n",
    "           'ANVIL_CMG_Yale_DS_BPEAKD_20240113_ANV5_202410011754',\n",
    "           'ANVIL_CMG_Yale_GRU_20221020_ANV5_202507091800'\n",
    "          ]\n",
    "\n",
    "for file in samples:\n",
    "    print(f\"\"\"\n",
    "    EXPORT DATA OPTIONS(uri='{bucket}/cmg_yale/sample_*.csv', format='CSV', overwrite=true ,header=true) AS \n",
    "    (\n",
    "    SELECT *\n",
    "    FROM `datarepo-5c7f0d2a.{file}.sample`\n",
    "    );\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = ['subject','sample','family','sequencing','file_inventory']\n",
    "\n",
    "datasets = ['datarepo-c2a2b724.ANVIL_CMG_YALE_DS_MC_20221026_ANV5_202409302315',\n",
    "           'datarepo-5c7f0d2a.ANVIL_CMG_Yale_HMB_IRB_20240113_ANV5_202410011846',\n",
    "           'datarepo-fe056168.ANVIL_CMG_Yale_DS_RD_20240113_ANV5_202410011804',\n",
    "           'datarepo-f961f617.ANVIL_CMG_YALE_DS_RARED_20221020_ANV5_202409251714',\n",
    "           'datarepo-06182245.ANVIL_CMG_Yale_DS_THAL_IRB_20240113_ANV5_202410011814',\n",
    "           'datarepo-35779fe0.ANVIL_CMG_Yale_HMB_20221020_ANV5_202410011825',\n",
    "           'datarepo-cebe6de0.ANVIL_CMG_Yale_HMB_GSO_20221020_ANV5_202410011834',\n",
    "           'datarepo-5d222190.ANVIL_CMG_Yale_DS_BPEAKD_20240113_ANV5_202410011754',\n",
    "           'datarepo-2b98851b.ANVIL_CMG_Yale_GRU_20221020_ANV5_202507091800'\n",
    "          ]\n",
    "\n",
    "for table in tables:\n",
    "    for ds in datasets:\n",
    "        tablename = ds.split('.', 1)[1]\n",
    "        print(f\"\"\"\n",
    "        EXPORT DATA OPTIONS(uri='{bucket}/{study_id}/{table}_{tablename}_*.csv', format='CSV', overwrite=true ,header=true) AS \n",
    "        (\n",
    "        SELECT *\n",
    "        FROM `{ds}.{table}`\n",
    "        );\n",
    "        \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in datasets:\n",
    "    tablename = ds.split('.', 1)[1]\n",
    "\n",
    "    print(f\"\"\"\n",
    "    EXPORT DATA OPTIONS(uri='{bucket}/cmg_yale/anvil_dataset_{tablename}_*.csv', format='CSV', overwrite=true ,header=true) AS \n",
    "    (\n",
    "      SELECT \n",
    "      dataset_id,\n",
    "      ARRAY_TO_STRING(consent_group, ',') AS consent_group,\n",
    "      ARRAY_TO_STRING(data_use_permission, ',') AS data_use_permission,\n",
    "      ARRAY_TO_STRING(owner, ',') AS owner,\n",
    "      ARRAY_TO_STRING(principal_investigator, ',') AS principal_investigator,\n",
    "      ARRAY_TO_STRING(registered_identifier, ',') AS registered_identifier,\n",
    "      title,\n",
    "      ARRAY_TO_STRING(data_modality, ',') AS data_modality,\n",
    "      ARRAY_TO_STRING(source_datarepo_row_ids, ',') AS source_datarepo_row_ids\n",
    "    FROM `{ds}.anvil_dataset`\n",
    "        );\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    for ds in datasets:\n",
    "        tablename = ds.split('.', 1)[1]\n",
    "        print(f\"\"\"\n",
    "        dbt run-operation register_external_sources --args '{{\"fq_tablename\": \"{table}_{tablename}_000000000000\", \"src_loc\": \"data/{study_id}/{table}_{tablename}_000000000000.csv\", \"src_format\": \"csv\"}}';\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (venv)",
   "language": "python",
   "name": "venv-python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
